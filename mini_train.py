# mini_train.py
import torch
from torch.utils.data import DataLoader
from models.conditional_unet import ConditionalUNet
from datasets.celeba_dataset import CelebADataset
from utils.flow_utils import flow_matching_loss
from utils.visualize import save_inpainting_result
import yaml
import os

def mini_batch_train():
    """
    å°æ‰¹é‡è®­ç»ƒä»£ç  - ç”¨äºå¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹
    """
    # åŠ è½½é…ç½®
    with open("config/config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # ä¿®æ”¹é…ç½®ä»¥è¿›è¡Œå°æ‰¹é‡è®­ç»ƒ
    config['num_epochs'] = 3  # åªè®­ç»ƒ3ä¸ªepoch
    config['batch_size'] = 4  # å°æ‰¹é‡
    config['img_size'] = 256  # è®¾ç½®å›¾åƒå°ºå¯¸ä¸º 256
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)
    os.makedirs(config['results_dir'], exist_ok=True)
    
    # åˆ›å»ºå°æ•°æ®é›†
    print("Loading dataset...")
    try:
        dataset = CelebADataset(config['data_root'], img_size=config['img_size'])
        print(f"Dataset loaded with {len(dataset)} samples")
    except Exception as e:
        print(f"Failed to load dataset: {e}")
        return
    
    # åˆ›å»ºå°æ‰¹é‡æ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=2  # å‡å°‘workeræ•°é‡
    )
    
    # åˆå§‹åŒ–æ¨¡å‹å¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    print("Initializing model...")
    model = ConditionalUNet(in_channels=3, width=64)  # æ˜ç¡®æŒ‡å®šå‚æ•°
    model = model.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    print(f"Model device: {next(model.parameters()).device}")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'] * 0.1)  # é™ä½å­¦ä¹ ç‡
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])
    
    print(f"ğŸš€ å¼€å§‹å°æ‰¹é‡è®­ç»ƒæµ‹è¯• ({config['num_epochs']} epochs)...")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['num_epochs']):
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        # åªå¤„ç†å‰å‡ ä¸ªbatchä»¥åŠ å¿«æµ‹è¯•
        for i, (masked, _, clean) in enumerate(dataloader):
            if i >= 5:  # åªå¤„ç†å‰5ä¸ªbatch
                break
                
            # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨ç›¸åŒè®¾å¤‡ä¸Š
            masked, clean = masked.to(device), clean.to(device)
            
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(masked).any() or torch.isnan(clean).any():
                print("NaN values detected in input, skipping batch...")
                continue
                
            # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
            try:
                # å‚æ•°é¡ºåº: model, ç›®æ ‡å›¾åƒ(clean), æ¡ä»¶å›¾åƒ(masked)
                # åœ¨ä¿®å¤ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä»maskedå›¾åƒç”Ÿæˆcleanå›¾åƒ
                loss = flow_matching_loss(model, clean, masked)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                    print("Invalid loss value, skipping batch...")
                    continue
                    
            except RuntimeError as e:
                print(f"Error in flow_matching_loss: {e}")
                print("Skipping this batch...")
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            print(f"  Batch {i+1}, Loss: {loss.item():.4f}")
        
        if num_batches > 0:
            avg_loss = total_loss / num_batches
            scheduler.step()
            
            print(f"Epoch [{epoch+1}/{config['num_epochs']}], Average Loss: {avg_loss:.4f}")
            
            # æ¯ä¸ªepochåä¿å­˜ä¸€æ¬¡ç»“æœ
            with torch.no_grad():
                try:
                    save_inpainting_result(
                        model,
                        (masked[:4].to(device), clean[:4].to(device)),  # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        device,
                        f"{config['results_dir']}/mini_batch_epoch_{epoch+1}.png"
                    )
                except Exception as e:
                    print(f"Failed to save visualization: {e}")
        else:
            print(f"Epoch [{epoch+1}/{config['num_epochs']}], No valid batches processed")
    
    # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
    try:
        torch.save(model.state_dict(), "checkpoints/mini_batch_model.pth")
        print("âœ… å°æ‰¹é‡è®­ç»ƒæµ‹è¯•å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ checkpoints/mini_batch_model.pth")
    except Exception as e:
        print(f"Failed to save model: {e}")

if __name__ == "__main__":
    mini_batch_train()